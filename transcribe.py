# -*- coding: utf-8 -*-
"""Copy of Whisperx diarize.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQ5IRpGMYPky3Lyyiahn2tmlvecvwFVU

A setup for automatic transcription with diarization using WhisperX: a Whisper based pipeline that does audio segmentation and transcription with word-level timestamps, voice recognition and diarization.

Github for WhisperX: https://github.com/m-bain/whisperX

Paper for WhisperX: https://arxiv.org/pdf/2303.00747.pdf

Use the "multiple file diarization" cell for transcribing. All you need to do is define the directory in which your files are.

Use the "single file diarization" cell mostly for testing and troubleshooting.

Depending on the quality of the audio file the system might produce some errors. They will be marked in the text file and indicate the time in the recording that it happened at.

If errors occur, a bit of audio editing on the file normally does the trick. Editing includes either noise reduction or level equalizing.

Diarization of noisy files is around 90% accurate, but it does require minimal cleanup. I haven't tested it on clean audio files - such as podcasts -  but I expected very good results.

If audio quality is really low - like in some of the interviews - the accuracy drops. Again, audio editing helps a lot. I'm working on an automated solution.

# Setup
"""

#mount your google drive if necessary
from google.colab import drive
drive.mount('/content/drive')

!pip install -U pyannote.audio

!pip install -U speechbrain

!pip install ctranslate2==4.4.0

# Uninstall existing torch and related libraries to start clean
!pip uninstall torch torchvision torchaudio -y
!pip install -q torch==2.5.1 torchvision==0.16.0 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118

# Uninstall existing torch and related libraries to start clean
#!pip uninstall torch torchvision torchaudio -y

# Uninstall potentially conflicting ctranslate2
#!pip uninstall ctranslate2 -y

# Install a known compatible version of torch with cu118.
# WhisperX documentation often suggests specific torch versions for optimal compatibility.
# As of recent whisperX versions, torch==2.1.0 with cu118 is a common requirement.
# If this still fails, you might need to check the whisperX documentation for the exact
# recommended torch version for the whisperx version you are using.
!pip install -q torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
# Install ctranslate2. It's a dependency for faster-whisper used by whisperx.
# A specific version is often required to match the faster-whisper version.
# Version 4.4.0 is often compatible with recent whisperX according to: https://github.com/m-bain/whisperX/issues/901
#!pip install ctranslate2==4.4.0

# Reinstall whisperx to ensure it picks up the correct dependencies.
!pip install git+https://github.com/m-bain/whisperx.git --upgrade

import torch # Import the torch module
import whisperx
import gc

# Print the CUDA version torch is built with for verification
print(f"Torch built with CUDA: {torch.version.cuda}")

torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

device = "cuda"
batch_size = 16 # this is the segmentation rate. We might play around with it to see how it affects accuracy. reduce if low on GPU mem
compute_type = "float16" # change to "int8" if low on GPU mem (may reduce accuracy)

#define and load whisperx model
model = whisperx.load_model("tiny", device=device, compute_type=compute_type)

#define some basic parameters

import datetime
from google.colab import files
def time(secs):
  return datetime.timedelta(seconds=round(secs))
import os

"""# Multiple file diarization"""

from pyannote.audio import Pipeline

def diarize(audio_file: str) -> dict:
    """
    Perform speaker diarization on an audio file using WhisperX for transcription and alignment,
    and pyannote.audio for speaker labeling.
    """

    # Step 1: Transcribe with WhisperX
    audio = whisperx.load_audio(audio_file)
    result = model.transcribe(audio, batch_size=batch_size, language="en")

    # Step 2: Align transcription
    model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
    result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

    # Step 3: Diarization using pyannote.audio (NOT whisperx!)
    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization@2.1",
        use_auth_token="REMOVED_TOKENTXLbBEiBLhOjsswkXlZAIiQKQpMGDsyFKp"
    )

    diarize_segments = pipeline(audio_file)

    # Step 4: Assign speakers using whisperx utility (if available)
    result = whisperx.assign_word_speakers(diarize_segments, result)

    return result

import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

#apply the function to multiple files from a directory of your choosing (please also define the path accordingly and pay attention to the "i" variable in the file name)

# define the directory path
dir_path = r'/content/drive/Shareddrives/Dedyk Thesis Work/SoGrape/Pilot Porto 2025/Pilot Porto 2025 Data /Upload videos/audio'
files = os.listdir(dir_path)


# Iterate directory.
for filename in files:
    # check if current path is a file
    path = os.path.join(dir_path, filename)
    filename_without_ext = os.path.splitext(os.path.basename(path))[0]
    isFile = os.path.isfile(path)
    if isFile:
      audio_file = path
      print(f"transcribing file {filename_without_ext}")

      # run diarization function for the current audio file
      result=diarize(audio_file)

      # write the results in a txt file
      f = open(f"/content/drive/Shareddrives/Dedyk Thesis Work/SoGrape/Pilot Porto 2025/Pilot Porto 2025 Data /Upload videos/audio/{filename_without_ext}.txt", "w")
      prev_speaker="none"
      text="none"
      for (d, segment) in enumerate(result["segments"]):
        if "speaker" in segment and segment["text"] != text:
          if segment["speaker"] != prev_speaker:   # checks that speakers do not get repeated in the same segment
            f.write("\n" + segment["speaker"] + '\n')
          pre_text=text
          text = segment["text"]
          if text[0] == ' ':
            text = text[1:]
          if pre_text[-1] != [' ']:
            f.write(' ' + text)
          else:
            f.write(text)
          prev_speaker=segment["speaker"]
        else:
          error_time=segment["start"]
          f.write('\n' + '\n' + (f"Error in diarizing, please check audio at time: {error_time}") + '\n' + '\n')
      f.close()

"""# Single file diarization - for testing and troubleshooting"""

!pip install -U speechbrain

import torch # Import the torch module

torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

# 0. load audio file - define your own file path
audio_file = "/content/drive/Shareddrives/Tasting-coffee/LSC 2024 data /Interviews /audio/168_23 Jun, 15.34​.m4a"

# 1. Transcribe with original whisper (batched) - whisperx already does audio segmentation to accomodate for larger audio files.
#    Segmentation is only at points in the audio where no words are spoken to ensure better transcribing and diarization.
#    Each segment is then transcribed using Whisper

audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)
#print(result["segments"]) # before alignment

# 2. Align whisper output - start and end time of each word is determined and aligned to each segment using phoneme classification
model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

#print(result["segments"]) # after alignment

# 3. Assign speaker labels - based on phoneme classification words in each segment get labeled according to their speakers
#    NOTE:  add your personal huggingface auth_token (note that you will have to consent to the pyannote terms and conditions first)
diarize_model = whisperx.DiarizationPipeline(model_name='pyannote/speaker-diarization@2.1', use_auth_token="REMOVED_TOKENjYmhMDMByImUbJLNAmtzMlQXJKIPNvzBqd", device=device)

# add min/max number of speakers if known
diarize_segments = diarize_model(audio_file)

result = whisperx.assign_word_speakers(diarize_segments, result)
#print(diarize_segments)
print(result["segments"]) # segments are now assigned speaker IDs

# write segments with assigned speaker to a text file - be sure to change file paths (if needed)
f = open("/content/drive/Shareddrives/Tasting-coffee/transcrpts/168_23 Jun, 15.34​.m4a", "w")
prev_speaker="none"
text="none"
for (i, segment) in enumerate(result["segments"]):
  if "speaker" in segment and segment["text"] != text:
      if segment["speaker"] != prev_speaker:   # checks that speakers do not get repeated in the same segment
        f.write("\n" + segment["speaker"] + '\n')
      pre_text=text
      text = segment["text"]
      if text[0] == ' ':
          text = text[1:]
      if pre_text[-1] != [' ']:
        f.write(' ' + text)
      else:
        f.write(text)
      prev_speaker=segment["speaker"]
  else:
      error_time=segment["start"]
      f.write('\n' + '\n' + (f"Error in diarizing, please check audio at time: {error_time}") + '\n' + '\n')
f.close()
#files.download("transcript.txt")